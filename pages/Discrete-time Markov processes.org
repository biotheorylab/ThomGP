* Here we systematically explain the theory of discrete-time Markov processes. For simplicity, we will restrict ourselves to finite state spaces and keep time finite so as to keep the exposition elementary.
* Let $S$ denote the state space, which, for now, will be a finite set.  Define the set of $n$-step histories as $H_{n} = \mathsf{Map}(\{0, 1, 2, \ldots n\}, S)$.  I.e. a history is a sequence of states labelled by times.  We may visualize a history as a path in the state space.  Optionally, we might endow $S$ with the structure of a graph and only take paths which lie in the graph as histories.
* A _stochastic process_ is a probability distribution $P \in \mathcal{D}(H_n)$ on histories.  Two important quantities associated to such a process are:
:PROPERTIES:
:END:
** The probability of being in a found in a state $q$ at a time $t$:
$$p(q,t) = P(\{h \in H_{n} \mid h(t) = q\})$$
** The probability of making a transition from state $q$ at time $t$ to state $q'$ at (a later) time $t$:
$$T(q', t' \mid q,t) = P(\{h \in H_{n} \mid h(t') = q'\} \mid \{h \in H_{n} \mid h(t) = q\})$$
* We now assume that the process is _one-step Markovian_, a.k.a. _memoryless_.  This means that future values only depend upon the present value and not on past values.  Formally, given any times $0 < t_1 < t_2 < t_3 < n$ and any $q_1, q_2, q_3 \in S$, we have
$$\begin{aligned} &P (\{h \in H_{n} \mid h(t_3) = q_3\} \mid \{h \in H_{n} \mid h(t) = q\}) = &P (\{h \in H_{n} \mid h(t_3) = q_3\} \mid \{h \in H_{n} \mid h(t) = q\}) $$